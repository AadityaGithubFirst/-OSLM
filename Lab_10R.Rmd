---
title: "Lab: Pure Prediction: KNN and SVM"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

# Data

Below we read in the breast-cancer dataset last seen in the PCA lab:
```{r}
df         <- read.csv("http://www.stat.cmu.edu/~pfreeman/breastcancer.csv",stringsAsFactors=TRUE)
response   <- df[,1]  # B for benign, M for malignant
predictors <- data.frame(scale(df[,-1]))
df         <- cbind(predictors,"Label"=response)
cat("Sample size: ",length(response),"\n")
```
These data reside on [Kaggle](https://www.kaggle.com/mciml/breast-cancer-wisconsin-data). They provide information on breast cancer tumors (read: features extracted from images of cells!) for 569 people in which malignancy was suspected. The data are marked by *extreme* multicollinearity and redundancy: bad for inference, but fine for prediction! You'll code KNN and SVM models for these data below.

**Note that I scaled (i.e., standardized) the predictor data frame.** This is advised for both KNN and SVM.

Also note: differentiating the benign and malignant tumors is pretty easy, so you will not see results that are substantially better, if at all better, than what you get when you learn a logistic regression model. The point today is the coding, not to get a reaction of "oh, wow, see how much better KNN and SVM do!"

## Question 1

Split the data and carry out a logistic regression analysis. (The response variable is dubbed `Label`.) Assume a class-separation threshold of 0.5, which is not optimal but good enough, particularly since changing that threshold in the context of KNN is difficult. (The optimal threshold would be nearer to 0.373. Why 0.373? The classes are imbalanced, and since `B` has more data (62.7% of the data) and is Class 0, the Class 1 probabilities will be systematically pulled downwards towards zero...and a decent guess at the optimal threshold would be 1 - 0.627 = 0.373.)
```{r}
set.seed(100)
s<-sample(nrow(df), nrow(df)*0.8)
df.train<-df[s,]
df.test<-df[-s,]
lm.out<-lm(Label~.,data = df.train)
```

## Question 2

Use the sample code in today's notes (altered for classification!...see Slide 10) to implement a KNN model. You will want to plot the validation-set MCR versus $k$. (Note: wherever it says `mse.k` in the notes, do `mcr.k` here...for "misclassification rate.") A value of `k.max` of 30 should be fine for you.

Note: the predictors are in columns 1-20 of `df.train` and `df.test`, and the response is in column 21.
```{r fig.align='center',fig.width=4,fig.height=4}
suppressMessages(library(tidyverse))
suppressMessages(library(class))
k.max <- 30
mcr.k <- numeric(k.max)
for (i in 1:k.max) {
  knn_pred <- knn(train = df.train[,1:20], 
                  test = df.test[,1:20], 
                  cl = df.train$Label, 
                  k = i)
  mcr.k[i] <- mean(knn_pred != df.test$Label)
}
k.opt <- which.min(mcr.k)
ggplot(data.frame(k = 1:k.max, mcr = mcr.k), aes(x = k, y = mcr)) +
  geom_line() +
  geom_point() +
  geom_point(aes(x = k.opt, y = mcr.k[k.opt]), color = "red", size = 3) +
  labs(title = "Validation-set Misclassification Rate vs. k",
       x = "k (Number of Neighbors)",
       y = "Misclassification Rate") +
  theme_minimal()
```

## Question 3

Re-run the `knn()` function so as to be able to extract Class 1 probabilities. As with Q2, here you are to reference Slide 10, but this time concentrate on adapting the code at the bottom. To demonstrate that you extracted the probabilities, simply histogram them. You should observe two clear peaks...one at 0, and one at 1.
```{r fig.align='center',fig.height=4,fig.width=4}
k.opt <- 8
probabilities <- numeric(nrow(df.test))  # Initialize vector for probabilities
for (i in 1:nrow(df.test)) {
  # Get the indices of the k nearest neighbors
  knn.out <- knn(train = df.train[, 1:20], 
                   test = df.train[, 1:20], 
                   cl = df.train$Label, 
                   k = k.opt,
                   prob = TRUE)
  knn.prob <- attributes(knn.out)$prob
  w <- which(knn.out=="B")
  knn.prob[w] <- 1 - knn.prob[w]
  
 
}

# Plot histogram of Class 1 probabilities
ggplot(data.frame(probability = knn.prob), aes(x = probability)) +
  geom_histogram(binwidth = 0.05, fill = "steelblue", color = "black") +
  geom_vline(xintercept = c(0, 1), linetype = "dashed", color = "red") +
  labs(title = "Histogram of KNN Class 1 Probabilities",
       x = "Probability of Class 1",
       y = "Frequency") +
  theme_minimal()
```

## Question 4

For SVM, we will work with the `e1071` package. (Its name comes from the coding for the Institute of Statistics and Probability Theory at the Technische Universitat Wien, in Vienna. It's like us calling a package `36-600`. Which we should.) Here, code a support vector classifier (meaning, do SVM with `kernel="linear"`): use the `tune()` function with a representative sequence of potential costs $C$, then extract the best model. If the optimum value of $C$ occurs at or very near the end of your sequence of potential costs, alter the sequence. The variable `best.parameters`, embedded in the output, provides the optimal value for $C$. Provide that value. Use the best model to generate predictions, a test-set MCR, and a confusion matrix.

Note that `tune()` does cross-validation on the training set to estimate the optimum value of $C$. Which means that the training data are randomly assigned to folds (by default, 10...to change this, you'd make a call like `tune.control(cross=5)`). Which means you should set a random number seed before calling `tune()`. For reproducibility n'at.

See the last code block of page 390 of `ISLR` (2nd edition) for an example of how to specify ranges of tuning parameters. Note there is only one here: `cost`. As for prediction: `tune()` will return an object that includes `best.model`. Pass this to `predict()` along with the argument `newdata=` whatever you call the test predictors data frame. By default, `predict()` will output a vector of class predictions, so there is no need to round off to determine classes.
```{r}
library(e1071)
set.seed(100)
tune.out <- tune(svm, Label ~ ., data = df.train,
                    kernel = "linear",
                    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100, 1000)),
                    tunecontrol = tune.control(cross = 5))

cat("The estimated optimal value for C is ",as.numeric(tune.out$best.parameters),"\n")
```

## Question 5

Now code a support vector machine with a polynomial kernel. In addition to tuning `cost`, you also have to tune the polynomial `degree`. Try integers from 2 up to some maximum number (not too large, like 4). (Note: if you get the warning `WARNING: reaching max number of iterations`, do not worry about it.)
```{r}
set.seed(100)
tune.out <- tune(svm,Label~.,data=df.train,kernel="polynomial",ranges=list(cost=10^seq(2,4,by=0.5),degree=2:4))
cat("The estimated optimal values for C and degree are ",as.numeric(tune.out$best.parameters),"\n")
```

## Question 6

Now code a support vector machine with a radial kernel. In addition to tuning `cost`, you also have to tune the parameter `gamma`. Try a base-10 logarithmic sequence of values that includes -8 (for $10^{-8}$).
```{r}
set.seed(100)
tune.out <- tune(svm,Label~.,data=df.train,kernel="radial",
ranges=list(cost=10^seq(-1,1,by=0.5),gamma=10^seq(-1,1,by=0.4)))
cat("The estimated optimal values for C and gamma are ",as.numeric(tune.out$best.parameters),"\n")
```
## Question 7

Re-run the `tune()` and `predict()` functions so as to be able to extract Class 1 probabilities. Reference the final bullet point on Slide 17. To demonstrate that you extracted the probabilities, simply histogram them. You should observe two clear peaks...one at 0, and one at 1.
```{r fig.align='center',fig.height=4,fig.width=4}
set.seed(100)
svm_model <- svm(Label ~ ., data = df.train, probability = TRUE, kernel = "radial")
probabilities <- attr(predict(svm_model, df.test[, 1:20], probability = TRUE), "probabilities")
hist(probabilities[, 2], main = "Histogram of Class 1 Probabilities", 
     xlab = "Probability of Class 1", col = "blue", breaks = 20)
```
