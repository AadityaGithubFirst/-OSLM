---
title: "Lab: Random Forest and Boosting"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

# Regression

We import the heart-disease dataset and log-transform the response variable, `Cost`:
```{r}
df      <- read.csv("http://www.stat.cmu.edu/~pfreeman/heart_disease.csv",stringsAsFactors=TRUE)
df      <- df[,-10]
w       <- which(df$Cost > 0)
df      <- df[w,]
df$Cost <- log(df$Cost)
```

## Question 1

Split these data into training and test sets, reusing the random-number-generator seed you used in previous labs when analyzing these data.
```{r}
set.seed(100)
s<-sample(nrow(df), round(nrow(df)*0.7))
df.train<-df[s,]
df.test<-df[-s,]
```

## Question 2

Learn a random forest model given the training data, and compute the MSE. Remember to set `importance=TRUE`. **Note: for reproducible results, set the seed before running random forest!** Assuming you split the data in the same manner as you did before, feel free to look back at your other labs and see if the MSE is smaller here. (For me and my split? It is...about 10% smaller than for a regression tree.)
```{r}
set.seed(100)
suppressMessages(library(randomForest))
rf.out<- randomForest(Cost~., df.train, importance= TRUE)
Cost2.pred<- predict(rf.out, df.test)
mean((df.test$Cost-Cost2.pred)^2)/nrow(df.test)
```

## Question 3

Create the variable importance plot. Remember to pass `type=1` as an argument to this plot. Mentally note the important variables. These should be consistent with those variables that appeared in your regression tree in the tree lab.
```{r fig.align='center',fig.height=4,fig.width=4}
varImpPlot(rf.out,type=1)
```

## Question 4

Show the diagnostic plot of predicted test-set response values vs. observed test-set response values. As usual, make sure the limits are the same along both axes and plot a diagonal line with slope 1.
```{r fig.align='center',fig.height=4,fig.width=4}
resp.pred = predict(rf.out,newdata=df.test)
suppressMessages(library(ggplot2))
df.plot <- data.frame("x"=resp.pred, "y" = df.test$Cost)
ggplot(data=df.plot,
mapping=aes(x=x,y=y)) +
geom_point(size=0.1,color="saddlebrown") +
xlim(0,11) + ylim(0,11) +
geom_abline(intercept=0,slope=1,color="red")
```

## Question 5

Now learn an extreme gradient boosting model, and show the test-set MSE. Note that in order to do this, we have to remove the variables `Gender`, `Drugs`, and `Complications`, which are factor or factor-like variables, and for ease of code implementation, we will break up `df.train` and `df.test` into predictor and response variables:
```{r}
library(dplyr)
df.train %>% dplyr::select(.,-Gender,-Drugs,-Complications) -> df.train
df.test  %>% dplyr::select(.,-Gender,-Drugs,-Complications) -> df.test
resp.train <- df.train[,1]
resp.test  <- df.test[,1]
pred.train <- df.train[,-1]
pred.test  <- df.test[,-1]
```
Note that by doing this, the MSE that we get might not be as good as for random forest. But we'll see!
```{r}
set.seed(100)
suppressMessages(library(xgboost))
train <- xgb.DMatrix(data=as.matrix(df.train[,2:6]),label=df.train$Cost)
test <- xgb.DMatrix(data=as.matrix(df.test[,2:6]),label=df.test$Cost)
xgb.cv.out <- xgb.cv(params=list(objective="reg:squarederror"),train,nrounds=30,nfold=5,verbose=0)
rmse.min <- xgb.cv.out$evaluation_log$test_rmse_mean
cat("The optimal number of trees is ",which.min(rmse.min),"\n")
xgb.out <- xgboost(train,nrounds=which.min(rmse.min),params=list(objective="reg:squarederror"),verbose=0)
resp.pred <- predict(xgb.out,newdata=test)
round(mean((resp.pred-df.test$Cost)^2),3)
```

## Question 6

Create a variable importance plot for the extreme gradient boosting model. Make a mental note about whether the variables identified as important here are also the more important ones identified by random forest.
```{r fig.align='center',fig.height=4,fig.width=4}
imp.out <- xgb.importance(model=xgb.out)
xgb.plot.importance(importance_matrix=imp.out)
```

---

# Classification

We will now load in the data on political movements that you looked at in previous labs:
```{r}
load(url("http://www.stat.cmu.edu/~pfreeman/movement.Rdata"))
f <- function(variable,level0="NO",level1="YES") {
  n               <- length(variable)
  new.variable    <- rep(level0,n)
  w               <- which(variable==1)
  new.variable[w] <- level1
  return(factor(new.variable))
}
predictors$nonviol      <- f(predictors$nonviol)
predictors$sanctions    <- f(predictors$sanctions)
predictors$aid          <- f(predictors$aid)
predictors$support      <- f(predictors$support)
predictors$viol.repress <- f(predictors$viol.repress)
predictors$defect       <- f(predictors$defect)
levels(response)        <- c("FAILURE","SUCCESS")
df           <- cbind(predictors,response)
names(df)[9] <- "label"
rm(id.half,id,predictors.half,predictors,response)
```

Note that given the number of factor variables in this dataset, we'll forego learning a boosting model below.

## Question 7

Split the data! Recreate what you did for previous labs, including the random-number-generator seed.
```{r}
set.seed(100)
s<-sample(nrow(df), round(nrow(df)*0.8))
df.train<-df[s,]
df.test<-df[-s,]
```

## Question 8

Learn a random forest model. Output probabilities for Class 1 (see the notes!) but do not output a confusion matrix or output a misclassification rate. It will become clear why we will hold off on computing this quantities for now... However, having said all this, do go ahead and plot the variable importance plot here.
```{r fig.align='center',fig.height=4,fig.width=4}
rf.out = randomForest(label~.,data=df.train,importance=TRUE)
out.pred = predict(rf.out,newdata=df.test,type="prob")[,2]
out.pred
```

## Question 9

Plot a ROC curve for random forest, and output the AUC value.
```{r fig.align='center',fig.height=4,fig.width=4}
suppressMessages(library(pROC))
resp.prob <- predict(rf.out,newdata=df.test, type = "prob")[,2]
roc.log <- roc(df.test$label,resp.prob)
roc.log
```

## Question 10

Use Youden's $J$ statistic to determine the optimal class-separation threshold. Output that number. Then, using that threshold, transform the test-set Class 1 probabilities to class predictions, and output the confusion matrix and the misclassification rate. (Note: you can reuse code from previous labs.)
```{r}
label.pred<- predict(rf.out, newdata = df.test, type = "class")
conf_matrix<-table(label.pred, df.test$label)
conf_matrix
tn <- conf_matrix[1,1]
fp <- conf_matrix[1,2]
fn <- conf_matrix[2,1]
tp <- conf_matrix[2,2]
J<-tp/(tp+fn)+tn/(tn+fp)-1
cat("Youden's statistic J is",J)
cat("\nThe misclassification rate is",(fp+fn)/length(label.pred) )
```
