---
title: "36-600-A_Project_1"
author: "Aaditya Hari Nair"
date: "2024-09-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Loading the dataset

The first step to EDA is to import the dataset into the programming language of choice otherwise there is no data to perform EDA with. 
In this step I also added all libraries that are going to be used during this entire Markdown file
I installed a package called hot.deck. In this package there is a function called is.discrete which basically tells whether a given data is discrete or not this is done by checking if there are more than 10 unique values. If there are not more than 10 unique values then it can be said that the given data is discrete as in there is only a finite set of values that each point in the data frame can fit into.
In the above cell I have added all the dependencies that 
From this we can see that there are some discrete columns in our table that means that these columns don't have too many values. These columns are study, cohort, demo_race, demo_gender and demo_firstgen.


```{r}
df<-read.csv("cmu-sleep.csv")
suppressMessages(library(hot.deck))
suppressMessages(library(tidyverse))
suppressMessages(library(corrplot))
summary(df)
head(df)
for (i in names(df)){
  if(is.discrete(df[i])==TRUE){
   print(i)
    }
}
```

I now use sapply to find all the columns that have got null values and find the number of rows that have null values. This is so that I can decide what is the next step that I should do.
Should I either:

1. Delete all the null value columns
2. Delete all the null value rows
3. Impute data into the null values using the median() or mean() function

You only use the first option if and only if you have so many null values in the column that honestly the amount of data that is understood from the columns is not a lot. You use the second method if there are only a few data points in the column that have a value of null.
We use the third option in most cases wherein there are very few rows that are null. We use median when we know that the data is categorical (i.e., it is discrete) and we use mean when the data is continous.

```{r}
sapply(df, function(x) sum(is.na(x)))
```
As it can be seen that term_units and zterm_units_ZofZ are both having a lot of null value we could drop these two rows. This is because there are around 634 observations and if we were to remove 147 rows of data it could cause some problems as the size fo the dataset is already so small. 
On top of that we also know that they are continuous variables making them difficult to easily impute with accurate values.
But I did perform imputations on the rows demo_race and demo_gender which I have imputed with the median value as the number of missing values is so low that it will cause little skewing to the data. The main reason I used median instead of mean for doing the imputation is because these rows are both discrete rows so it would be better suited to having a median number rather than the mean that could be any number that is in between the two numbers. 
I also found that the cohort column is a character column which is going to make it difficult to find the correlation matrix for the dataset. As a result I have replaced all the values in the cohort column with number this is due to cohort having a finite set of values (being discrete) so factoring it was possible.
I then checked my dataset to see if there were any null values that were remaining.

```{r}
df = subset(df, select = -c(term_units, Zterm_units_ZofZ, subject_id))
df$demo_race[is.na(df$demo_race)] <- median(df$demo_race,na.rm = TRUE)
df$demo_gender[is.na(df$demo_gender)] <- median(df$demo_gender,na.rm = TRUE)
df$demo_firstgen[is.na(df$demo_firstgen)] <- median(df$demo_firstgen,na.rm = TRUE)
df$cohort<-  as.integer(factor(df$cohort))
sapply(df, function(x) sum(is.na(x)))
```

```{r}
field<-setdiff(names(df), c("study", "cohort", "demo_race", "demo_gender", "demo_firstgen"))
df.quant <- gather(select(df, field))
ggplot(data=df.quant,mapping=aes(x=value)) +
  geom_histogram(fill="deepskyblue3",bins=30) +
  facet_wrap(~key,scales='free')
```
From these graphs it can be seen that some of the values in the graph are not present which is causing some skewing in the data. In the first graph it can be seen that a lot of the responses in that tab show people having not missed a single bedtime which is causing the graph to skew left. 
Now to plot the graphs of those values that are discrete
```{r}
disc<- setdiff(names(df), field)
df.disc <- gather(df %>% select(disc))
ggplot(data=df.disc,mapping=aes(x=value)) +
  geom_bar(fill="deeppink3") +
  facet_wrap(~key,scales='free')
```

From the second graph it can be seen that there are very few rows that have a value of 2 in demo_firstgen.
Therefore it could be proposed that all rows in the table wherein which the demo_firstgen is 2 can be dropped. But it could also be replaced (imputed) with another value so as to actually save that data
```{r}
df.quant<-df %>% select(setdiff(field,"cum_gpa"))
df.new<-scale(df.quant)
correl<- cor(df.new)
corrplot(correl, method = "square")
corrplot(correl, method = "number")
```


I have done two visualizations of the correlation matrix. One of them is the correlation matrix which uses the square method to actually use the visual properties of a graph to be able to better understand it.
I have done a secondary graph which has numbers this is so that I can get a better understanding of the correlation values of each column with the other columns.

Now let me try it out in a different method wherein which rather than deleting the two columns with values term_units and Zterm_units_Z_of_Z I rather create two dataframes. One with term_units and Zterm_units_Z_of_Z and the other without these two units and redo all of the EDA again to check whether these columns are actually required


```{r}
df<-read.csv("cmu-sleep.csv")
df.withnona<-df %>% drop_na(term_units)
df.withnona$demo_race[is.na(df.withnona$demo_race)] <- median(df.withnona$demo_race,na.rm = TRUE)
df.withnona$demo_gender[is.na(df.withnona$demo_gender)] <- median(df.withnona$demo_gender,na.rm = TRUE)
df.withnona$demo_firstgen[is.na(df.withnona$demo_firstgen)] <- median(df.withnona$demo_firstgen,na.rm = TRUE)
df.withnona$cohort<-  as.integer(factor(df.withnona$cohort))
sapply(df.withnona, function(x) sum(is.na(x)))
head(df.withnona)
```

```{r}
field<-setdiff(names(df.withnona), c("study", "cohort", "demo_race", "demo_gender", "demo_firstgen"))
df.quant <- gather(df.withnona %>% select(field))
ggplot(data=df.quant,mapping=aes(x=value)) +
  geom_histogram(fill="deepskyblue3",bins=30) +
  facet_wrap(~key,scales='free')
```
```{r}
disc<- setdiff(names(df), field)
df.disc <- gather(df.withnona %>% select(disc))
ggplot(data=df.disc,mapping=aes(x=value)) +
  geom_bar(fill="deeppink3") +
  facet_wrap(~key,scales='free')
```

From this it can be concluded that if you drop all the rows that have null values then a lot of rows which are part of study four have been removed which could cause you to think that it would be a good reason to remove the study group four. Also just like previously there is only a singular point in demo_firstgen that has value 2 so that value can be removed as it is an outlier. Also it means that 



```{r}
df.quant<-df.withnona %>% select(setdiff(field,"cum_gpa"))
df.new<-scale(df.quant)
correl<- cor(df.new)
corrplot(correl, method = "square")
corrplot(correl, method = "number")
```


F



























