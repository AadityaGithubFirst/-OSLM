---
title: "Lab: Data Manipulation with dplyr"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

## Data

We'll begin by importing some astronomical data from the 36-290 GitHub site. These data are stored in `.Rdata` format; such data are saved via `R`'s `save()` function and loaded via `R`'s `load()` function. One wrinkle here: the data are stored on the web, so we also have to apply the `url()` function.
```{r}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/BUZZARD/Buzzard_DC1.Rdata"))
set.seed(101)
s  <- sample(nrow(df),4000)
df <- df[s,-c(7:13)]
```

`df` is a data frame with 4000 rows and 7 columns; the first six are predictor variables and the seventh, `redshift`, is the response variable. Redshift is a directly observable proxy for the distance of a galaxy from the Earth. (After all, tape measures aren't going to help us here.) In a statistical learning exercise, we might try to determine a statistical model that optimally associates the predictor variables with the `redshift`.

# Questions

## Question 1

Apply the `dim()`, `nrow()`, `ncol()`, and `length()` functions to `df`, so as to build intuition about what these functions output. (Note: we are not using `dplyr` yet, just base `R` functions here.) Do you know why `length()` returns the value it does? Ask me or the TA if you do not. (But fill in an answer below regardless to reinforce the answer.)
```{r}
dim(df)
nrow(df)
ncol(df)
length(df)
```
```
The reason why the function length() gives back seven is because of the fact that the way that r stores dataframes is in such a way that it stores in a matrix format and r can only read the second value of the matrix 
```

## Question 2

Display the names of each column of `df`.
```{r}
names(df)
```

---

Time for a digression.

A *magnitude* is a logarithmic measure of brightness that is calibrated such as to be approximately zero for the brightest stars in the night sky (Sirius, Vega, etc.). Every five-unit *increase* in magnitude is a factor of 100 *decrease* in brightness. So a magnitude of 20 means the object is 100 million times fainter than a star like Vega.

Magnitudes are generally measured in particular bands. Imagine that you put a filter in front of a telescope that only lets photons of certain wavelengths pass through. You can then assess the brightness of an object at just those wavelengths. So `u` represents a magnitude determined at ultraviolet wavelengths, with `g`, `r`, and `i` representing green, red, and infrared. (The `z` and `y` are a bit further into the infrared. The names don't represent words.)

So the predictor data consists of six magnitudes spanning from the near-UV to the near-IR.

---

## Question 3

Use the base `R` `summary()` function to get a textual summary of the data frame. Do you notice anything strange? (Some values you might not expect?)
```{r}
summary(df)
```
```
From the summary it can be seen that the dataframe df has got some outliers being that they have some columns like u, g, z and y that have max values of 99 which seems to be a bit skewed due to the fact that the the minimum mean and median are so small 
```

---

## dplyr

Below, we will practice using `dplyr` package functions to select rows and/or columns of a data frame. `dplyr` is part of the `tidyverse`, and is rapidly becoming the most oft-used way of transforming data. To learn more about "transformatory" functions, read through today's class notes, then through Chapter 5 of the online version of *R for Data Science* (see the syllabus for the link, or just Google for it). In short, you can

| action | function |
| ------ | -------- |
| pick features by name | `select()` |
| pick observations by value | `filter()` |
| pick observations by category | `group_by()` |
| create new features | `mutate()` |
| reorder rows | `arrange()` |
| collapse rows to summaries | `summarize()` |

A cool thing about `dplyr` is that you can use a piping operator (%&gt;%) to have the output of one function be the input to the next. And you don't have to have only `dplyr` functions within the flow; for instance, you could pipe the first two rows your data frame to head:
```{r}
suppressMessages(library(tidyverse))
head(df[,1:2])                         # base R
df %>% select(.,u,g) %>% head(.)       # dplyr 
```

Let's do a few exercises here. Be sure to tap into, e.g., StackOverflow and *R for Data Science* for any help you may need.

---

## Question 4

Grab all data for which `i` is less than 25 and `g` is greater than 22, and output in order of increasing `y`. (Remember: you combine conditions with &amp; for "and" and | for "or".) Show only the first six lines of output. Note that `head()` by default shows the first six rows of the input data frame.
```{r}
result <- df[df$i < 25 & df$g > 22, ]
typeof(df$y)
result_sorted <- arrange(df, df$y)
head(result_sorted)
```

## Question 5

To get a quick, textual idea of how the data are distributed: select the `g` column, pipe it to the `round()` function, then pipe the output of `round()` to `table()`. You should notice something strange about the output...the same thing you should have noticed when answering Question 3.
```{r}
table(round(df$g))
```

---

Time for another digression. Domain scientists are not bound by the `R` convention of using `NA` when data are "not available," or missing. Sometimes the domain scientists will tell you what they use in place of `NA`, sometimes not. In those latter cases, one can usual infer the values. Astronomers for some reason love using -99, -9, 99, etc., to represent missing values. The values of 99 in the table you generated in Question 5 actually represent missing data.

We could change the values of 99 to `NA`, but here we will just filter those rows with values of 99 out of the data frame.

---

## Question 6

Use `filter()` to determine how many rows contain 99's. Since 99's can appear in different columns, you will need to combine conditions together with logical "and"s or "or"s. Note: only four of the six columns have 99's in them.
```{r}
nrow(filter(df, u==99| g==99 | z==99| y==99))
```

## Question 7

Now repeat Question 6 (in a sense), and remove all the rows that contain 99's, saving the new data frame as `df.new`. Hint that may help: the opposite of, e.g., `u==99 | g==99` is `u!=99 & g!=99`, etc. If you apply `dim()` to the new data frame, you should see that 3607 rows are retained.
```{r}
df.new<-filter(df, u!=99&g!=99&z!=99&y!=99)
dim(df.new)
```

---

The data we are working with have no factor variables, so I'm going to create one on the fly:
```{r}
type <- rep("FAINT",nrow(df.new))
w <- which(df.new$i<25)
type[w] <- "BRIGHT"
type <- factor(type)
unique(type)
df.new <- cbind(type,df.new)
```
So I defined my factor variable using character strings, and then coerced the vector of strings into a factor variable with two levels. Note that by default, the levels order themselves into alphabetical order. You can override that default behavior if there is actually a natural ordering to your factor variables. See the documentation for `factor()` to see how to do that.

---

## Question 8

Use `group_by()` and `summarize()` to determine the numbers of `BRIGHT` and `FAINT` galaxies in the dataset. (If you are adventuresome: try implementing the `tally()` function instead of `summarize()`. For our purposes here, it's actually easier.)
```{r}
df.new%>%group_by(type)%>%summarize(count = n())
```

## Question 9

Repeat Question 8, but show the median value of the `u` magnitude instead of the numbers in each factor group. (You do need `summarize()` here; `tally()` won't help you.)
```{r}
df.new%>%group_by(type)%>%summarize(median(u))
```

---

Time for yet another digression.

Magnitudes of galaxies at particular wavelengths are heavily influenced by two factors: physics (what is going on with the gas, dust, and stars within the galaxy itself), and distance (the further away a galaxy is, the less bright it tends to be, so the magnitude generally goes up). To attempt to mitigate (somewhat, not necessarily completely) the effect of distance, astronomers often use *colors*, which are differences in magnitude for two adjacent filters.

---

## Question 10

Use `mutate()` to define two new columns for `df.new`: `gr`, which would be the `g` magnitude minus the `r` magnitude, and `ri`, for `r` and `i`. Save your result as `df.newer`.
```{r}
gr<-(df.new$g-df.new$r)
df.newer<-df.new%>%mutate(gr=g-r, ri=r-i)
```

## Question 11

Are the mean values of `g-r` and `r-i` roughly the same for `BRIGHT` galaxies versus `FAINT` ones? Heck if I know. Use `dplyr` functions to attempt to answer this question.
```{r}
df.newer%>%group_by(type)%>%summarize(mean(gr), mean(ri))
```

## Question 12

Let's go back to hypothesis testing for a moment. Let's extract two vectors of data from `df.newer`:
```{r}
gr.faint  <- df.newer$gr[df.newer$type=="FAINT"]
gr.bright <- df.newer$gr[df.newer$type=="BRIGHT"]
```
The first vector has all the `g-r` colors for faint galaxies, and the second vector has all the ones for bright galaxies. The code here *should* be familiar given what we covered in Week 01, but if you are not sure what this code is doing, call me or the TA over.

Here are the histograms for `gr.faint` and `gr.bright`:
```{r}
hist(gr.faint,prob=TRUE,main=NULL,col=alpha("firebrick",0.4),ylim=c(0,1.2),xlab="color",breaks=seq(-1.5,5.5,by=0.2))
hist(gr.bright,prob=TRUE,main=NULL,col=alpha("dodgerblue",0.4),breaks=seq(-1.5,5.5,by=0.2),add=TRUE)
```

Now: test the hypothesis that the means of the distributions that `gr.faint` and `gr.bright` are sampled from are the same. Use a two-sample $t$ test. What do you conclude? Is a two-sample $t$ test actually strictly appropriate here? How would you know? (Even if the answer is no, do the $t$ test anyway.)
```{r}
t.test(gr.faint, gr.bright, alternative = 'two.sided')
```
```
From the t-test we can concluded that we reject the null hypothesis and we can say that the means of gr.faint and gr.bright are not sampled from the same place. The two sample t-test may be right but not for this question due to the fact that the two sample t-test outlines only areas around 2.5% either side which could cause skewing in this case due to the fact that the most of the data is towards the centre of the graph.
```

